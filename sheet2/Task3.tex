According to Occam's razor Theorem:
\\
\bigbreak
$ m \geq \frac{1}{\epsilon} (n\ln{\mid \Sigma \mid}+\ln{\frac{2}{\delta}})$
\\
\bigbreak
Being:
\begin{itemize}
	\item $\epsilon = 0.05$
	\item $\delta = 0.2$
\end{itemize}
As soon as decision trees can be described with:\\
$n = K(\log{K}+\log{l}))$ bits\\
\bigbreak
when:
\begin{itemize}
	\item $K = 16$ (number of nodes)
	\item $l = 6$ (number of features)
	\item $\Sigma = \{0,1\} => \mid \Sigma \mid = 2$)
\end{itemize}
Then:\\
\bigbreak
	$m \geq \frac{1}{\epsilon} (K(\log{K}+\log{l})\log{2}+\log{\frac{2}{\frac{2}{10}}})$\\
	\bigbreak
	$ m \geq \lceil 20\cdot (16(\log{(16\cdot 6)}\cdot \log{2}+\log{10})) \rceil = 1059 $\\
	\bigbreak
Finally:\\
The lower bound on the number of training examples is approximately $\textbf{1059}$. 
